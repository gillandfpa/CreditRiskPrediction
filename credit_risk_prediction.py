# -*- coding: utf-8 -*-
"""Credit Risk Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tIMLBr_SmlRa2f3cgQkkQsY3H5BYAnDg

# Import Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
pd.set_option('display.max_rows', 200)
pd.set_option('display.max_columns', 200)

"""# Load Dataset"""

df = pd.read_csv('loan_data_2007_2014.csv')

"""# Exploring Data"""

df.shape

df.info()

"""* Dataframe memiliki total 466285 baris dan 75 kolom
* Dataframe masih memiliki *null* values di beberapa kolom
* Target klasifikasi adalah kolom `loan_status` dengan tipe data `object`
* Sisanya adalah *feature* (predictor)


"""

df.sample(5)

df.id.nunique()

df.member_id.nunique()

"""Terlihat bahwa tidak ada `id` atau `member_id` yang duplikat

Selanjutnya, pembuangan fitur-fitur yang tidak berguna. Contohnya seperti fitur yang merupakan id unik, berupa free text, nilainya kosong semua
"""

col_to_drop = [
    # unique id
    'id' , 'member_id'
    
    # free text
    , 'url' , 'desc'
    
    # all null / constant / others
    , 'zip_code' , 'annual_inc_joint' , 'dti_joint' , 'verification_status_joint' , 'open_acc_6m' , 'open_il_6m'
    , 'open_il_12m' , 'open_il_24m' , 'mths_since_rcnt_il' , 'total_bal_il' , 'il_util' , 'open_rv_12m' , 'open_rv_24m'
    , 'max_bal_bc' , 'all_util' , 'inq_fi' , 'total_cu_tl' , 'inq_last_12m'
    
    # expert judgment
    , 'sub_grade'
]

data = df.drop(col_to_drop, axis=1)

data.sample(5)

"""# Define Target Variabel

Dalam dataset ini, variabel `loan_status` adalah variabel yang dapat dijadikan variabel target karena mencerminkan performa masing-masing individu dalam melakukan pembayaran terhadap pinjaman/kredit selama ini.
"""

data['loan_status'].unique()

data['loan_status'].value_counts()

loan_c = data['loan_status'].value_counts()
loan_c = loan_c[:10,]
plt.figure(figsize=(15,5))
sns.barplot(loan_c.values, loan_c.index, alpha=0.8)
plt.title('Loan Condition')
plt.ylabel('Loans Status', fontsize=12)
plt.xlabel('Count', fontsize=12)
plt.show()

"""* Terdapat 9 nilai unik pada kolom `loan_status` yang akan menjadi target model.
* Dibagi menjadi dua kelompok, yaitu "good_loan" dengan angka 1 dan "bad_loan" dengan angka 0
* "good_loan" didefinisikan memiliki status `Current`, `Fully Paid`, dan `In Grace Period`
* "bad_loan" didefinisikan sebagai status selain dari "good_loan"

Mengelompokkan `loan_condition` menjadi good_loan dan bad_loan
"""

bad_status = [
    'Charged Off' 
    , 'Default' 
    , 'Does not meet the credit policy. Status:Charged Off'
    , 'Does not meet the credit policy. Status:Fully Paid'
    , 'Late (16-30 days)'
    , 'Late (31-120 days)'
]

data['bad_flag'] = np.where(data['loan_status'].isin(bad_status), 1, 0)

data.drop('loan_status', axis=1, inplace=True)

"""# CLEANING, PREPROCESSING, FEATURE ENGINEERING

Pada step ini, dilakukan pembersihan/modifikasi beberapa fitur ke dalam format yang dapat digunakan untuk modeling.

### emp_length

Memodifikasi `emp_length`. Contoh: 4 years -> 4
"""

data['emp_length'].unique()

data['emp_length_int'] = data['emp_length'].str.replace('\+ years', '')
data['emp_length_int'] = data['emp_length_int'].str.replace('< 1 year', str(0))
data['emp_length_int'] = data['emp_length_int'].str.replace(' years', '')
data['emp_length_int'] = data['emp_length_int'].str.replace(' year', '')

data['emp_length_int'] = data['emp_length_int'].astype(float)

data.drop('emp_length', axis=1, inplace=True)

"""### term

Memodifikasi `term`. Contoh: 36 months -> 36
"""

data['term'].unique()

data['term_int'] = data['term'].str.replace(' months', '')
data['term_int'] = data['term_int'].astype(float)

data.drop('term', axis=1, inplace=True)

"""### earliest_cr_line

Memodifikasi `earliest_cr_line` dari format bulan-tahun menjadi perhitungan berapa lama waktu berlalu sejak waktu tersebut. Untuk melakukan hal ini, umumnya digunakan reference date = hari ini. Namun, karena dataset ini merupakan dataset tahun 2007-2014, maka akan lebih relevan jika menggunakan reference date di sekitar tahun 2017. Dalam contoh ini, saya menggunakan tanggal 2017-12-01 sebagai reference date.
"""

data['earliest_cr_line'].head(5)

data['earliest_cr_line_date'] = pd.to_datetime(data['earliest_cr_line'], format='%b-%y')
data['earliest_cr_line_date'].head(5)

data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['earliest_cr_line_date']) / np.timedelta64(1, 'M')))
data['mths_since_earliest_cr_line'].head(5)

data['mths_since_earliest_cr_line'].describe()

"""Terlihat ada nilai yang aneh, yaitu negatif."""

data[data['mths_since_earliest_cr_line']<0][['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']].head(5)

"""Ternyata nilai negatif muncul karena fungsi Python salah menginterpretasikan tahun 62 menjadi tahun 2062, padahal seharusnya merupakan tahun 1962.

Untuk mengatasi hal ini, dapat dilakukan preprocessing lebih jauh jika ingin membenarkan tahun 2062 menjadi 1962. Namun, kali ini saya hanya mengubah nilai yang negatif menjadi nilai maximum dari fitur tersebut. Karena di sini saya mengetahui bahwa nilai-nilai yang negatif artinya adalah data yang sudah tua (tahun 1900an), maka masih masuk akal jika saya mengganti nilai-nilai tersebut menjadi nilai terbesar.
"""

data.loc[data['mths_since_earliest_cr_line']<0, 'mths_since_earliest_cr_line'] = data['mths_since_earliest_cr_line'].max()

data.drop(['earliest_cr_line', 'earliest_cr_line_date'], axis=1, inplace=True)

"""### issue_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['issue_d_date'] = pd.to_datetime(data['issue_d'], format='%b-%y')
data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['issue_d_date']) / np.timedelta64(1, 'M')))

data.drop(['issue_d', 'issue_d_date'], axis=1, inplace=True)

"""### last_pymnt_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['last_pymnt_d_date'] = pd.to_datetime(data['last_pymnt_d'], format='%b-%y')
data['mths_since_last_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['last_pymnt_d_date']) / np.timedelta64(1, 'M')))

data['mths_since_last_pymnt_d'].describe()

data.drop(['last_pymnt_d', 'last_pymnt_d_date'], axis=1, inplace=True)

"""### next_pymnt_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['next_pymnt_d_date'] = pd.to_datetime(data['next_pymnt_d'], format='%b-%y')
data['mths_since_next_pymnt_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['next_pymnt_d_date']) / np.timedelta64(1, 'M')))

data['mths_since_next_pymnt_d'].describe()

data.drop(['next_pymnt_d', 'next_pymnt_d_date'], axis=1, inplace=True)

"""### last_credit_pull_d

Konsep preprocessing yang dilakukan sama dengan yang dilakukan terhadap variabel `earliest_cr_line`
"""

data['last_credit_pull_d_date'] = pd.to_datetime(data['last_credit_pull_d'], format='%b-%y')
data['mths_since_last_credit_pull_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - data['last_credit_pull_d_date']) / np.timedelta64(1, 'M')))

data['mths_since_last_credit_pull_d'].describe()

data.drop(['last_credit_pull_d', 'last_credit_pull_d_date'], axis=1, inplace=True)

data.info()

"""Dari informasi di atas, kita bisa pisahkan kolom *numerical* dan *categorical* sebagai berikut:"""

nums = [key for key in dict(data.dtypes) if dict(data.dtypes)[key] in ['float64', 'int64']]
cats = [key for key in dict(data.dtypes) if dict(data.dtypes)[key] in ['object']]

"""## Descriptive Statistics

Ringkasan statistik dataframe untuk *numerical*
"""

data[nums].describe()

"""Ringkasan statistik dataframe untuk *categorical*"""

data[cats].describe()

"""* Beberapa fitur kategori memiliki terlalu banyak nilai unik dan hanya memiliki satu nilai unik

## Univariate Analysis

Dist Plots
"""

plt.figure(figsize=(24,28))
for i in range(0,len(nums)):
    plt.subplot(10,4,i+1)
    sns.kdeplot(x=data[nums[i]], palette='viridis', shade=True)
    plt.title(nums[i], fontsize=20)
    plt.xlabel(' ')
    plt.tight_layout()

"""* Mayoritas fitur *numerical* tidak terdistribusi secara normal

## Multivariate Analysis

Correlation Heatmap
"""

plt.figure(figsize=(20, 20))
sns.heatmap(data.corr(), cmap='Greens', annot=True, fmt='.1f')

"""* Tampaknya ada beberapa fitur independen yang sangat berkorelasi positif kuat satu sama lain, kemungkinan redundan

Di sini, jika ada pasangan fitur-fitur yang memiliki korelasi tinggi maka akan diambil salah satu saja. Nilai korelasi yang dijadikan patokan sebagai korelasi tinggi tidak pasti, umumnya digunakan angka 0.7.
"""

corr_matrix = data.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
to_drop_hicorr = [column for column in upper.columns if any(upper[column] > 0.7)]

to_drop_hicorr

data.drop(to_drop_hicorr, axis=1, inplace=True)

"""### Check Categorical Features"""

data.select_dtypes(include='object').nunique()

"""Pada tahap ini dilakukan pembuangan fitur yang memiliki nilai unik yang sangat tinggi (high cardinality) dan fitur yang hanya memiliki satu nilai unik saja."""

data.drop(['emp_title', 'title', 'application_type'], axis=1, inplace=True)

data.select_dtypes(exclude='object').nunique()

"""Ternyata, pada tipe data selain `object` juga terdapat fitur yang hanya memiliki satu nilai unik saja, maka akan ikut dibuang juga."""

data.drop(['policy_code'], axis=1, inplace=True)

for col in data.select_dtypes(include='object').columns.tolist():
    print(data[col].value_counts(normalize=True)*100)
    print('\n')

"""Fitur yang sangat didominasi oleh salah satu nilai saja akan dibuang pada tahap ini."""

data.drop('pymnt_plan', axis=1, inplace=True)

"""# MISSING VALUES

Memeriksa nilai-nilai yang hilang
"""

check_missing = data.isnull().sum() * 100 / data.shape[0]
check_missing[check_missing > 0].sort_values(ascending=False)

"""kolom-kolom dengan missing values di atas 75% dibuang"""

data.drop('mths_since_last_record', axis=1, inplace=True)

"""### Missing Values Filling"""

data['annual_inc'].fillna(data['annual_inc'].mean(), inplace=True)
data['mths_since_earliest_cr_line'].fillna(0, inplace=True)
data['acc_now_delinq'].fillna(0, inplace=True)
data['total_acc'].fillna(0, inplace=True)
data['pub_rec'].fillna(0, inplace=True)
data['open_acc'].fillna(0, inplace=True)
data['inq_last_6mths'].fillna(0, inplace=True)
data['delinq_2yrs'].fillna(0, inplace=True)
data['collections_12_mths_ex_med'].fillna(0, inplace=True)
data['revol_util'].fillna(0, inplace=True)
data['emp_length_int'].fillna(0, inplace=True)
data['tot_cur_bal'].fillna(0, inplace=True)
data['tot_coll_amt'].fillna(0, inplace=True)
data['mths_since_last_delinq'].fillna(-1, inplace=True)

"""# FEATURE SCALING AND TRANSFORMATION

Semua kolom kategorikal dilakukan One Hot Encoding
"""

categorical_cols = [col for col in data.select_dtypes(include='object').columns.tolist()]

onehot = pd.get_dummies(data[categorical_cols], drop_first=True)

onehot.head()

"""### Standardization

Semua kolom numerikal dilakukan proses standarisasi dengan StandardScaler.
"""

numerical_cols = [col for col in data.columns.tolist() if col not in categorical_cols + ['bad_flag']]

from sklearn.preprocessing import StandardScaler

ss = StandardScaler()
std = pd.DataFrame(ss.fit_transform(data[numerical_cols]), columns=numerical_cols)

std.head()

"""### Transformed Dataframe

Menggabungkan kembali kolom-kolom hasil transformasi 
"""

data_model = pd.concat([onehot, std, data[['bad_flag']]], axis=1)

"""# MODELING

### Train-Test Split
"""

from sklearn.model_selection import train_test_split

X = data_model.drop('bad_flag', axis=1)
y = data_model['bad_flag']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

"""### Training

Pada contoh ini digunakan algoritma Random Forest untuk pemodelan.
"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(max_depth=4)
rfc.fit(X_train, y_train)

"""Feature Importance dapat ditampilkan."""

arr_feature_importances = rfc.feature_importances_
arr_feature_names = X_train.columns.values
    
df_feature_importance = pd.DataFrame(index=range(len(arr_feature_importances)), columns=['feature', 'importance'])
df_feature_importance['feature'] = arr_feature_names
df_feature_importance['importance'] = arr_feature_importances
df_all_features = df_feature_importance.sort_values(by='importance', ascending=False)
df_all_features

"""### Validation

Untuk mengukur performa model, dua metrik yang umum dipakai dalam dunia credit risk adalah AUC dan KS
"""

y_pred_proba = rfc.predict_proba(X_test)[:][:,1]

df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)
df_actual_predicted.index = y_test.index

"""AUC"""

from sklearn.metrics import roc_curve, roc_auc_score

fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])
auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])

plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)
plt.plot(fpr, fpr, linestyle = '--', color='k')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()

"""### KS"""

df_actual_predicted = df_actual_predicted.sort_values('y_pred_proba')
df_actual_predicted = df_actual_predicted.reset_index()

df_actual_predicted['Cumulative N Population'] = df_actual_predicted.index + 1
df_actual_predicted['Cumulative N Bad'] = df_actual_predicted['y_actual'].cumsum()
df_actual_predicted['Cumulative N Good'] = df_actual_predicted['Cumulative N Population'] - df_actual_predicted['Cumulative N Bad']
df_actual_predicted['Cumulative Perc Population'] = df_actual_predicted['Cumulative N Population'] / df_actual_predicted.shape[0]
df_actual_predicted['Cumulative Perc Bad'] = df_actual_predicted['Cumulative N Bad'] / df_actual_predicted['y_actual'].sum()
df_actual_predicted['Cumulative Perc Good'] = df_actual_predicted['Cumulative N Good'] / (df_actual_predicted.shape[0] - df_actual_predicted['y_actual'].sum())

df_actual_predicted.head()

KS = max(df_actual_predicted['Cumulative Perc Good'] - df_actual_predicted['Cumulative Perc Bad'])

plt.plot(df_actual_predicted['y_pred_proba'], df_actual_predicted['Cumulative Perc Bad'], color='r')
plt.plot(df_actual_predicted['y_pred_proba'], df_actual_predicted['Cumulative Perc Good'], color='b')
plt.xlabel('Estimated Probability for Being Bad')
plt.ylabel('Cumulative %')
plt.title('Kolmogorov-Smirnov:  %0.4f' %KS)

"""Model yang dibangun menghasilkan performa `AUC = 0.8516` dan `KS = 0.5599`. Pada dunia credit risk modeling, umumnya AUC di atas 0.7 dan KS di atas 0.3 sudah termasuk performa yang baik."""